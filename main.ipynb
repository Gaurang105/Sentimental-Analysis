{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\GAURANG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\GAURANG\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to open: scraped-articles\\37.txt\n",
      "Trying to open: scraped-articles\\38.txt\n",
      "Trying to open: scraped-articles\\39.txt\n",
      "Trying to open: scraped-articles\\40.txt\n",
      "Trying to open: scraped-articles\\41.txt\n",
      "Trying to open: scraped-articles\\42.txt\n",
      "Trying to open: scraped-articles\\43.txt\n",
      "File scraped-articles\\44.txt not found, skipping.\n",
      "Trying to open: scraped-articles\\45.txt\n",
      "Trying to open: scraped-articles\\46.txt\n",
      "Trying to open: scraped-articles\\47.txt\n",
      "Trying to open: scraped-articles\\48.txt\n",
      "Trying to open: scraped-articles\\49.txt\n",
      "Trying to open: scraped-articles\\50.txt\n",
      "Trying to open: scraped-articles\\51.txt\n",
      "Trying to open: scraped-articles\\52.txt\n",
      "Trying to open: scraped-articles\\53.txt\n",
      "Trying to open: scraped-articles\\54.txt\n",
      "Trying to open: scraped-articles\\55.txt\n",
      "Trying to open: scraped-articles\\56.txt\n",
      "File scraped-articles\\57.txt not found, skipping.\n",
      "Trying to open: scraped-articles\\58.txt\n",
      "Trying to open: scraped-articles\\59.txt\n",
      "Trying to open: scraped-articles\\60.txt\n",
      "Trying to open: scraped-articles\\61.txt\n",
      "Trying to open: scraped-articles\\62.txt\n",
      "Trying to open: scraped-articles\\63.txt\n",
      "Trying to open: scraped-articles\\64.txt\n",
      "Trying to open: scraped-articles\\65.txt\n",
      "Trying to open: scraped-articles\\66.txt\n",
      "Trying to open: scraped-articles\\67.txt\n",
      "Trying to open: scraped-articles\\68.txt\n",
      "Trying to open: scraped-articles\\69.txt\n",
      "Trying to open: scraped-articles\\70.txt\n",
      "Trying to open: scraped-articles\\71.txt\n",
      "Trying to open: scraped-articles\\72.txt\n",
      "Trying to open: scraped-articles\\73.txt\n",
      "Trying to open: scraped-articles\\74.txt\n",
      "Trying to open: scraped-articles\\75.txt\n",
      "Trying to open: scraped-articles\\76.txt\n",
      "Trying to open: scraped-articles\\77.txt\n",
      "Trying to open: scraped-articles\\78.txt\n",
      "Trying to open: scraped-articles\\79.txt\n",
      "Trying to open: scraped-articles\\80.txt\n",
      "Trying to open: scraped-articles\\81.txt\n",
      "Trying to open: scraped-articles\\82.txt\n",
      "Trying to open: scraped-articles\\83.txt\n",
      "Trying to open: scraped-articles\\84.txt\n",
      "Trying to open: scraped-articles\\85.txt\n",
      "Trying to open: scraped-articles\\86.txt\n",
      "Trying to open: scraped-articles\\87.txt\n",
      "Trying to open: scraped-articles\\88.txt\n",
      "Trying to open: scraped-articles\\89.txt\n",
      "Trying to open: scraped-articles\\90.txt\n",
      "Trying to open: scraped-articles\\91.txt\n",
      "Trying to open: scraped-articles\\92.txt\n",
      "Trying to open: scraped-articles\\93.txt\n",
      "Trying to open: scraped-articles\\94.txt\n",
      "Trying to open: scraped-articles\\95.txt\n",
      "Trying to open: scraped-articles\\96.txt\n",
      "Trying to open: scraped-articles\\97.txt\n",
      "Trying to open: scraped-articles\\98.txt\n",
      "Trying to open: scraped-articles\\99.txt\n",
      "Trying to open: scraped-articles\\100.txt\n",
      "Trying to open: scraped-articles\\101.txt\n",
      "Trying to open: scraped-articles\\102.txt\n",
      "Trying to open: scraped-articles\\103.txt\n",
      "Trying to open: scraped-articles\\104.txt\n",
      "Trying to open: scraped-articles\\105.txt\n",
      "Trying to open: scraped-articles\\106.txt\n",
      "Trying to open: scraped-articles\\107.txt\n",
      "Trying to open: scraped-articles\\108.txt\n",
      "Trying to open: scraped-articles\\109.txt\n",
      "Trying to open: scraped-articles\\110.txt\n",
      "Trying to open: scraped-articles\\111.txt\n",
      "Trying to open: scraped-articles\\112.txt\n",
      "Trying to open: scraped-articles\\113.txt\n",
      "Trying to open: scraped-articles\\114.txt\n",
      "Trying to open: scraped-articles\\115.txt\n",
      "Trying to open: scraped-articles\\116.txt\n",
      "Trying to open: scraped-articles\\117.txt\n",
      "Trying to open: scraped-articles\\118.txt\n",
      "Trying to open: scraped-articles\\119.txt\n",
      "Trying to open: scraped-articles\\120.txt\n",
      "Trying to open: scraped-articles\\121.txt\n",
      "Trying to open: scraped-articles\\122.txt\n",
      "Trying to open: scraped-articles\\123.txt\n",
      "Trying to open: scraped-articles\\124.txt\n",
      "Trying to open: scraped-articles\\125.txt\n",
      "Trying to open: scraped-articles\\126.txt\n",
      "Trying to open: scraped-articles\\127.txt\n",
      "Trying to open: scraped-articles\\128.txt\n",
      "Trying to open: scraped-articles\\129.txt\n",
      "Trying to open: scraped-articles\\130.txt\n",
      "Trying to open: scraped-articles\\131.txt\n",
      "Trying to open: scraped-articles\\132.txt\n",
      "Trying to open: scraped-articles\\133.txt\n",
      "Trying to open: scraped-articles\\134.txt\n",
      "Trying to open: scraped-articles\\135.txt\n",
      "Trying to open: scraped-articles\\136.txt\n",
      "Trying to open: scraped-articles\\137.txt\n",
      "Trying to open: scraped-articles\\138.txt\n",
      "Trying to open: scraped-articles\\139.txt\n",
      "Trying to open: scraped-articles\\140.txt\n",
      "Trying to open: scraped-articles\\141.txt\n",
      "Trying to open: scraped-articles\\142.txt\n",
      "Trying to open: scraped-articles\\143.txt\n",
      "File scraped-articles\\144.txt not found, skipping.\n",
      "Trying to open: scraped-articles\\145.txt\n",
      "Trying to open: scraped-articles\\146.txt\n",
      "Trying to open: scraped-articles\\147.txt\n",
      "Trying to open: scraped-articles\\148.txt\n",
      "Trying to open: scraped-articles\\149.txt\n",
      "Trying to open: scraped-articles\\150.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import openpyxl\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load Input.xlsx file\n",
    "input_wb = openpyxl.load_workbook('Input.xlsx')\n",
    "input_ws = input_wb.active\n",
    "\n",
    "# Load or create Output.xlsx file\n",
    "output_wb = openpyxl.Workbook()\n",
    "output_ws = output_wb.active\n",
    "output_ws.append(['URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE', \n",
    "                  'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', 'WORD COUNT', \n",
    "                  'COMPLEX WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'])\n",
    "\n",
    "# Folder with articles and positive-negative words\n",
    "articles_dir = 'scraped-articles'\n",
    "pos_neg_words_dir = 'positive-negative words'\n",
    "\n",
    "# Load positive and negative words\n",
    "with open(os.path.join(pos_neg_words_dir, 'positive-words.txt'), 'r', encoding='utf-8') as f:\n",
    "    positive_words = set(f.read().splitlines())\n",
    "with open(os.path.join(pos_neg_words_dir, 'negative-words.txt'), 'r', encoding='utf-8') as f:\n",
    "    negative_words = set(f.read().splitlines())\n",
    "\n",
    "# Define stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define function to count syllables in a word\n",
    "def syllable_count(word):\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    vowels = \"aeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index-1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith(\"e\"):\n",
    "        count -= 1\n",
    "    if word.endswith(\"le\"):\n",
    "        count += 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "# Process each row in the input worksheet\n",
    "for row in input_ws.iter_rows(min_row=2, values_only=True):\n",
    "    url_id, url = row\n",
    "    filename = f\"{int(url_id)}.txt\"\n",
    "    filepath = os.path.join(articles_dir, filename)\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f'File {filepath} not found, skipping.')\n",
    "        continue\n",
    "\n",
    "    print(f'Trying to open: {filepath}')\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Tokenize text\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Calculate scores\n",
    "    positive_score = sum(word in positive_words for word in tokens)\n",
    "    negative_score = sum(word in negative_words for word in tokens)\n",
    "\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score) / ((len(tokens)) + 0.000001)\n",
    "\n",
    "    # Readability calculations\n",
    "    average_sentence_length = len(tokens) / len(sentences)\n",
    "    complex_words_count = sum(1 for word in tokens if syllable_count(word) > 2)\n",
    "    percentage_complex_words = complex_words_count / len(tokens)\n",
    "    fog_index = 0.4 * (average_sentence_length + percentage_complex_words)\n",
    "    \n",
    "    # Word count after removing stop words and punctuation\n",
    "    cleaned_words = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    word_count = len(cleaned_words)\n",
    "\n",
    "    # Syllables per word\n",
    "    syllables_per_word = sum(syllable_count(word) for word in cleaned_words) / word_count\n",
    "    \n",
    "    # Personal pronouns\n",
    "    personal_pronouns_count = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.IGNORECASE))\n",
    "\n",
    "    # Average word length\n",
    "    average_word_length = sum(len(word) for word in cleaned_words) / word_count\n",
    "\n",
    "    # Append results to output worksheet\n",
    "    output_ws.append([url_id, url, positive_score, negative_score, polarity_score, subjectivity_score, \n",
    "                      average_sentence_length, percentage_complex_words, fog_index, word_count, \n",
    "                      complex_words_count, syllables_per_word, personal_pronouns_count, average_word_length])\n",
    "\n",
    "# Save output workbook\n",
    "output_wb.save('Output Data Structure.xlsx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
